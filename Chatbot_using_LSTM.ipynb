{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVuZTAV08qWY"
      },
      "source": [
        "# Step 1: Import all the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0mJXRse83hp"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras import layers, activations, models, preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL0Rz6JZ9eLW"
      },
      "source": [
        "# Step 2: Download all the data from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cU-sP689zyx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "dded5b3f-878a-4b1e-e812-7492251af341"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-620b384b-cffa-42f0-8db4-b52741368824\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-620b384b-cffa-42f0-8db4-b52741368824\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-5c2e8a8d365b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    157\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    158\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWYOjzOc_iQi"
      },
      "source": [
        "from tensorflow.keras import preprocessing, utils\n",
        "import os\n",
        "import yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip chatbot_nlp.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDMqrapZ7eh-",
        "outputId": "84710529-016a-4373-d8f2-1ad40ad665dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  chatbot_nlp.zip\n",
            "   creating: chatbot_nlp/\n",
            "   creating: chatbot_nlp/data/\n",
            "  inflating: chatbot_nlp/data/ai.yml  \n",
            "  inflating: chatbot_nlp/data/botprofile.yml  \n",
            "  inflating: chatbot_nlp/data/computers.yml  \n",
            "  inflating: chatbot_nlp/data/emotion.yml  \n",
            "  inflating: chatbot_nlp/data/food.yml  \n",
            "  inflating: chatbot_nlp/data/gossip.yml  \n",
            "  inflating: chatbot_nlp/data/greetings.yml  \n",
            "  inflating: chatbot_nlp/data/health.yml  \n",
            "  inflating: chatbot_nlp/data/history.yml  \n",
            "  inflating: chatbot_nlp/data/humor.yml  \n",
            "  inflating: chatbot_nlp/data/literature.yml  \n",
            "  inflating: chatbot_nlp/data/money.yml  \n",
            "  inflating: chatbot_nlp/data/movies.yml  \n",
            "  inflating: chatbot_nlp/data/politics.yml  \n",
            "  inflating: chatbot_nlp/data/psychology.yml  \n",
            "  inflating: chatbot_nlp/data/science.yml  \n",
            "  inflating: chatbot_nlp/data/sports.yml  \n",
            "  inflating: chatbot_nlp/data/trivia.yml  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxG-s4k0CowI"
      },
      "source": [
        "dir_path = '/content/chatbot_nlp/data'\n",
        "files_list = os.listdir(dir_path + os.sep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bRvbQ00Coy5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84b641c9-d850-40b5-e4fa-a0f7c385769e"
      },
      "source": [
        "questions = list()\n",
        "answers = list()\n",
        "\n",
        "for filepath in files_list:\n",
        "    stream = open( dir_path + os.sep + filepath , 'rb')\n",
        "    docs = yaml.safe_load(stream)\n",
        "    conversations = docs['conversations']\n",
        "    for con in conversations:\n",
        "        if len( con ) > 2 :\n",
        "            questions.append(con[0])\n",
        "            replies = con[ 1 : ]\n",
        "            ans = ''\n",
        "            for rep in replies:\n",
        "                ans += ' ' + rep\n",
        "            answers.append( ans )\n",
        "        elif len( con )> 1:\n",
        "            questions.append(con[0])\n",
        "            answers.append(con[1])\n",
        "\n",
        "answers_with_tags = list()\n",
        "for i in range( len( answers ) ):\n",
        "    if type( answers[i] ) == str:\n",
        "        answers_with_tags.append( answers[i] )\n",
        "    else:\n",
        "        questions.pop( i )\n",
        "\n",
        "answers = list()\n",
        "for i in range( len( answers_with_tags ) ) :\n",
        "    answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )\n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( questions + answers )\n",
        "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
        "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VOCAB SIZE : 1894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMPqb8LxIeGI"
      },
      "source": [
        "###  Preparing data for Seq2Seq model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEfAPL4HCo1t"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqYoDsbSCo4f"
      },
      "source": [
        "vocab = []\n",
        "for word in tokenizer.word_index:\n",
        "  vocab.append(word)\n",
        "\n",
        "def tokenize(sentences):\n",
        "  tokens_list = []\n",
        "  vocabulary = []\n",
        "  for sentence in sentences:\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "    tokens = sentence.split()\n",
        "    vocabulary += tokens\n",
        "    tokens_list.append(tokens)\n",
        "  return tokens_list, vocabulary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vKhieIwCo7J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f01bcabb-7a54-4840-9a02-c0385e91512f"
      },
      "source": [
        "#encoder_input_data\n",
        "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
        "maxlen_questions = max( [len(x) for x in tokenized_questions ] )\n",
        "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions, maxlen = maxlen_questions, padding = 'post')\n",
        "encoder_input_data = np.array(padded_questions)\n",
        "print(encoder_input_data.shape, maxlen_questions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(564, 22) 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJo7WPjLCo-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7eeb728-84ba-4eed-c13d-53bf1a52e1d3"
      },
      "source": [
        "# decoder_input_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
        "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
        "decoder_input_data = np.array( padded_answers )\n",
        "print( decoder_input_data.shape , maxlen_answers )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(564, 74) 74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccY0wWdRCpCa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fefed878-3437-48f8-b96f-7856a7eb449c"
      },
      "source": [
        "# decoder_output_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
        "for i in range(len(tokenized_answers)) :\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
        "onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
        "decoder_output_data = np.array( onehot_answers )\n",
        "print( decoder_output_data.shape )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(564, 74, 1894)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D53pyucPCnk"
      },
      "source": [
        "# Step 4: Defining Encoder Decoder Model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3YjCFDwPRVN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7210323d-fbd6-490f-cdfa-cab18e402b16"
      },
      "source": [
        "encoder_inputs = tf.keras.layers.Input(shape=( maxlen_questions , ))\n",
        "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
        "encoder_states = [ state_h , state_c ]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=( maxlen_answers ,  ))\n",
        "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
        "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
        "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax )\n",
        "output = decoder_dense ( decoder_outputs )\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 22)]                 0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 74)]                 0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 22, 200)              378800    ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, 74, 200)              378800    ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 [(None, 200),                320800    ['embedding[0][0]']           \n",
            "                              (None, 200),                                                        \n",
            "                              (None, 200)]                                                        \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               [(None, 74, 200),            320800    ['embedding_1[0][0]',         \n",
            "                              (None, 200),                           'lstm[0][1]',                \n",
            "                              (None, 200)]                           'lstm[0][2]']                \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 74, 1894)             380694    ['lstm_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1779894 (6.79 MB)\n",
            "Trainable params: 1779894 (6.79 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVfSormAPb3w"
      },
      "source": [
        "# Step 5: Training the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHlqQq64PYTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7c5a097-edf9-4808-efb5-a980f7c24b13"
      },
      "source": [
        "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=50, epochs=150 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "12/12 [==============================] - 15s 176ms/step - loss: 7.5268\n",
            "Epoch 2/150\n",
            "12/12 [==============================] - 2s 161ms/step - loss: 6.8178\n",
            "Epoch 3/150\n",
            "12/12 [==============================] - 2s 223ms/step - loss: 5.9921\n",
            "Epoch 4/150\n",
            "12/12 [==============================] - 1s 115ms/step - loss: 5.8378\n",
            "Epoch 5/150\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 5.7808\n",
            "Epoch 6/150\n",
            "12/12 [==============================] - 1s 116ms/step - loss: 5.7445\n",
            "Epoch 7/150\n",
            "12/12 [==============================] - 1s 112ms/step - loss: 5.7233\n",
            "Epoch 8/150\n",
            "12/12 [==============================] - 1s 81ms/step - loss: 5.6987\n",
            "Epoch 9/150\n",
            "12/12 [==============================] - 1s 110ms/step - loss: 5.6664\n",
            "Epoch 10/150\n",
            "12/12 [==============================] - 1s 84ms/step - loss: 5.6368\n",
            "Epoch 11/150\n",
            "12/12 [==============================] - 1s 121ms/step - loss: 5.5988\n",
            "Epoch 12/150\n",
            "12/12 [==============================] - 1s 90ms/step - loss: 5.5694\n",
            "Epoch 13/150\n",
            "12/12 [==============================] - 2s 124ms/step - loss: 5.5281\n",
            "Epoch 14/150\n",
            "12/12 [==============================] - 1s 73ms/step - loss: 5.4882\n",
            "Epoch 15/150\n",
            "12/12 [==============================] - 1s 73ms/step - loss: 5.4411\n",
            "Epoch 16/150\n",
            "12/12 [==============================] - 1s 59ms/step - loss: 5.3900\n",
            "Epoch 17/150\n",
            "12/12 [==============================] - 1s 68ms/step - loss: 5.3779\n",
            "Epoch 18/150\n",
            "12/12 [==============================] - 1s 69ms/step - loss: 5.3428\n",
            "Epoch 19/150\n",
            "12/12 [==============================] - 1s 82ms/step - loss: 5.3138\n",
            "Epoch 20/150\n",
            "12/12 [==============================] - 1s 77ms/step - loss: 5.2903\n",
            "Epoch 21/150\n",
            "12/12 [==============================] - 1s 78ms/step - loss: 5.2813\n",
            "Epoch 22/150\n",
            "12/12 [==============================] - 1s 69ms/step - loss: 5.2642\n",
            "Epoch 23/150\n",
            "12/12 [==============================] - 1s 76ms/step - loss: 5.2277\n",
            "Epoch 24/150\n",
            "12/12 [==============================] - 1s 57ms/step - loss: 5.2164\n",
            "Epoch 25/150\n",
            "12/12 [==============================] - 1s 50ms/step - loss: 5.2058\n",
            "Epoch 26/150\n",
            "12/12 [==============================] - 1s 93ms/step - loss: 5.1738\n",
            "Epoch 27/150\n",
            "12/12 [==============================] - 1s 90ms/step - loss: 5.1643\n",
            "Epoch 28/150\n",
            "12/12 [==============================] - 1s 49ms/step - loss: 5.1437\n",
            "Epoch 29/150\n",
            "12/12 [==============================] - 1s 51ms/step - loss: 5.1163\n",
            "Epoch 30/150\n",
            "12/12 [==============================] - 1s 61ms/step - loss: 5.0937\n",
            "Epoch 31/150\n",
            "12/12 [==============================] - 1s 45ms/step - loss: 5.0659\n",
            "Epoch 32/150\n",
            "12/12 [==============================] - 1s 54ms/step - loss: 5.0400\n",
            "Epoch 33/150\n",
            "12/12 [==============================] - 1s 45ms/step - loss: 5.0251\n",
            "Epoch 34/150\n",
            "12/12 [==============================] - 0s 41ms/step - loss: 5.0026\n",
            "Epoch 35/150\n",
            "12/12 [==============================] - 1s 49ms/step - loss: 4.9699\n",
            "Epoch 36/150\n",
            "12/12 [==============================] - 0s 38ms/step - loss: 4.9470\n",
            "Epoch 37/150\n",
            "12/12 [==============================] - 0s 36ms/step - loss: 4.9307\n",
            "Epoch 38/150\n",
            "12/12 [==============================] - 0s 39ms/step - loss: 4.9031\n",
            "Epoch 39/150\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 4.8838\n",
            "Epoch 40/150\n",
            "12/12 [==============================] - 1s 43ms/step - loss: 4.8534\n",
            "Epoch 41/150\n",
            "12/12 [==============================] - 1s 52ms/step - loss: 4.8373\n",
            "Epoch 42/150\n",
            "12/12 [==============================] - 1s 55ms/step - loss: 4.8044\n",
            "Epoch 43/150\n",
            "12/12 [==============================] - 1s 69ms/step - loss: 4.7784\n",
            "Epoch 44/150\n",
            "12/12 [==============================] - 1s 61ms/step - loss: 4.7707\n",
            "Epoch 45/150\n",
            "12/12 [==============================] - 1s 51ms/step - loss: 4.7481\n",
            "Epoch 46/150\n",
            "12/12 [==============================] - 1s 59ms/step - loss: 4.7214\n",
            "Epoch 47/150\n",
            "12/12 [==============================] - 1s 76ms/step - loss: 4.7012\n",
            "Epoch 48/150\n",
            "12/12 [==============================] - 1s 47ms/step - loss: 4.6893\n",
            "Epoch 49/150\n",
            "12/12 [==============================] - 1s 69ms/step - loss: 4.6528\n",
            "Epoch 50/150\n",
            "12/12 [==============================] - 1s 43ms/step - loss: 4.6358\n",
            "Epoch 51/150\n",
            "12/12 [==============================] - 0s 42ms/step - loss: 4.6227\n",
            "Epoch 52/150\n",
            "12/12 [==============================] - 0s 36ms/step - loss: 4.6050\n",
            "Epoch 53/150\n",
            "12/12 [==============================] - 1s 47ms/step - loss: 4.5757\n",
            "Epoch 54/150\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 4.5557\n",
            "Epoch 55/150\n",
            "12/12 [==============================] - 0s 40ms/step - loss: 4.5405\n",
            "Epoch 56/150\n",
            "12/12 [==============================] - 0s 32ms/step - loss: 4.5134\n",
            "Epoch 57/150\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 4.4991\n",
            "Epoch 58/150\n",
            "12/12 [==============================] - 0s 40ms/step - loss: 4.4780\n",
            "Epoch 59/150\n",
            "12/12 [==============================] - 1s 48ms/step - loss: 4.4468\n",
            "Epoch 60/150\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 4.4245\n",
            "Epoch 61/150\n",
            "12/12 [==============================] - 1s 54ms/step - loss: 4.3847\n",
            "Epoch 62/150\n",
            "12/12 [==============================] - 1s 45ms/step - loss: 4.3883\n",
            "Epoch 63/150\n",
            "12/12 [==============================] - 0s 42ms/step - loss: 4.3677\n",
            "Epoch 64/150\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 4.3555\n",
            "Epoch 65/150\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 4.3345\n",
            "Epoch 66/150\n",
            "12/12 [==============================] - 1s 36ms/step - loss: 4.3078\n",
            "Epoch 67/150\n",
            "12/12 [==============================] - 1s 47ms/step - loss: 4.2902\n",
            "Epoch 68/150\n",
            "12/12 [==============================] - 1s 49ms/step - loss: 4.2627\n",
            "Epoch 69/150\n",
            "12/12 [==============================] - 1s 42ms/step - loss: 4.2618\n",
            "Epoch 70/150\n",
            "12/12 [==============================] - 1s 54ms/step - loss: 4.2328\n",
            "Epoch 71/150\n",
            "12/12 [==============================] - 1s 51ms/step - loss: 4.2044\n",
            "Epoch 72/150\n",
            "12/12 [==============================] - 1s 41ms/step - loss: 4.1944\n",
            "Epoch 73/150\n",
            "12/12 [==============================] - 0s 37ms/step - loss: 4.1880\n",
            "Epoch 74/150\n",
            "12/12 [==============================] - 0s 42ms/step - loss: 4.1526\n",
            "Epoch 75/150\n",
            "12/12 [==============================] - 0s 32ms/step - loss: 4.1414\n",
            "Epoch 76/150\n",
            "12/12 [==============================] - 0s 41ms/step - loss: 4.1243\n",
            "Epoch 77/150\n",
            "12/12 [==============================] - 0s 36ms/step - loss: 4.1132\n",
            "Epoch 78/150\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 4.0879\n",
            "Epoch 79/150\n",
            "12/12 [==============================] - 0s 32ms/step - loss: 4.0753\n",
            "Epoch 80/150\n",
            "12/12 [==============================] - 1s 40ms/step - loss: 4.0262\n",
            "Epoch 81/150\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 4.0314\n",
            "Epoch 82/150\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 4.0204\n",
            "Epoch 83/150\n",
            "12/12 [==============================] - 1s 53ms/step - loss: 3.9939\n",
            "Epoch 84/150\n",
            "12/12 [==============================] - 0s 40ms/step - loss: 3.9846\n",
            "Epoch 85/150\n",
            "12/12 [==============================] - 0s 40ms/step - loss: 3.9591\n",
            "Epoch 86/150\n",
            "12/12 [==============================] - 0s 39ms/step - loss: 3.9398\n",
            "Epoch 87/150\n",
            "12/12 [==============================] - 0s 39ms/step - loss: 3.9263\n",
            "Epoch 88/150\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 3.9042\n",
            "Epoch 89/150\n",
            "12/12 [==============================] - 1s 47ms/step - loss: 3.8679\n",
            "Epoch 90/150\n",
            "12/12 [==============================] - 1s 50ms/step - loss: 3.8711\n",
            "Epoch 91/150\n",
            "12/12 [==============================] - 0s 38ms/step - loss: 3.8483\n",
            "Epoch 92/150\n",
            "12/12 [==============================] - 0s 42ms/step - loss: 3.8418\n",
            "Epoch 93/150\n",
            "12/12 [==============================] - 0s 40ms/step - loss: 3.8293\n",
            "Epoch 94/150\n",
            "12/12 [==============================] - 1s 52ms/step - loss: 3.8104\n",
            "Epoch 95/150\n",
            "12/12 [==============================] - 1s 43ms/step - loss: 3.7918\n",
            "Epoch 96/150\n",
            "12/12 [==============================] - 1s 42ms/step - loss: 3.7756\n",
            "Epoch 97/150\n",
            "12/12 [==============================] - 1s 60ms/step - loss: 3.7486\n",
            "Epoch 98/150\n",
            "12/12 [==============================] - 1s 43ms/step - loss: 3.7407\n",
            "Epoch 99/150\n",
            "12/12 [==============================] - 0s 42ms/step - loss: 3.7198\n",
            "Epoch 100/150\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 3.6920\n",
            "Epoch 101/150\n",
            "12/12 [==============================] - 0s 37ms/step - loss: 3.6884\n",
            "Epoch 102/150\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 3.6732\n",
            "Epoch 103/150\n",
            "12/12 [==============================] - 0s 36ms/step - loss: 3.6508\n",
            "Epoch 104/150\n",
            "12/12 [==============================] - 1s 48ms/step - loss: 3.6408\n",
            "Epoch 105/150\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 3.6130\n",
            "Epoch 106/150\n",
            "12/12 [==============================] - 0s 39ms/step - loss: 3.6028\n",
            "Epoch 107/150\n",
            "12/12 [==============================] - 0s 36ms/step - loss: 3.5838\n",
            "Epoch 108/150\n",
            "12/12 [==============================] - 0s 39ms/step - loss: 3.5563\n",
            "Epoch 109/150\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 3.5528\n",
            "Epoch 110/150\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 3.5318\n",
            "Epoch 111/150\n",
            "12/12 [==============================] - 0s 41ms/step - loss: 3.5158\n",
            "Epoch 112/150\n",
            "12/12 [==============================] - 1s 43ms/step - loss: 3.4905\n",
            "Epoch 113/150\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 3.4871\n",
            "Epoch 114/150\n",
            "12/12 [==============================] - 0s 39ms/step - loss: 3.4628\n",
            "Epoch 115/150\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 3.4516\n",
            "Epoch 116/150\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 3.4417\n",
            "Epoch 117/150\n",
            "12/12 [==============================] - 1s 39ms/step - loss: 3.4167\n",
            "Epoch 118/150\n",
            "12/12 [==============================] - 0s 40ms/step - loss: 3.3982\n",
            "Epoch 119/150\n",
            "12/12 [==============================] - 1s 43ms/step - loss: 3.3730\n",
            "Epoch 120/150\n",
            "12/12 [==============================] - 0s 39ms/step - loss: 3.3676\n",
            "Epoch 121/150\n",
            "12/12 [==============================] - 1s 65ms/step - loss: 3.3561\n",
            "Epoch 122/150\n",
            "12/12 [==============================] - 1s 44ms/step - loss: 3.3401\n",
            "Epoch 123/150\n",
            "12/12 [==============================] - 1s 46ms/step - loss: 3.3324\n",
            "Epoch 124/150\n",
            "12/12 [==============================] - 1s 43ms/step - loss: 3.2985\n",
            "Epoch 125/150\n",
            "12/12 [==============================] - 0s 33ms/step - loss: 3.2979\n",
            "Epoch 126/150\n",
            "12/12 [==============================] - 1s 42ms/step - loss: 3.2797\n",
            "Epoch 127/150\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 3.2515\n",
            "Epoch 128/150\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 3.2505\n",
            "Epoch 129/150\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 3.2236\n",
            "Epoch 130/150\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 3.2087\n",
            "Epoch 131/150\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 3.1979\n",
            "Epoch 132/150\n",
            "12/12 [==============================] - 0s 39ms/step - loss: 3.1791\n",
            "Epoch 133/150\n",
            "12/12 [==============================] - 0s 41ms/step - loss: 3.1506\n",
            "Epoch 134/150\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 3.1418\n",
            "Epoch 135/150\n",
            "12/12 [==============================] - 0s 36ms/step - loss: 3.1311\n",
            "Epoch 136/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 3.1078\n",
            "Epoch 137/150\n",
            "12/12 [==============================] - 0s 36ms/step - loss: 3.0821\n",
            "Epoch 138/150\n",
            "12/12 [==============================] - 0s 32ms/step - loss: 3.0774\n",
            "Epoch 139/150\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 3.0712\n",
            "Epoch 140/150\n",
            "12/12 [==============================] - 0s 32ms/step - loss: 3.0530\n",
            "Epoch 141/150\n",
            "12/12 [==============================] - 0s 37ms/step - loss: 3.0380\n",
            "Epoch 142/150\n",
            "12/12 [==============================] - 0s 37ms/step - loss: 3.0229\n",
            "Epoch 143/150\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 3.0076\n",
            "Epoch 144/150\n",
            "12/12 [==============================] - 0s 37ms/step - loss: 2.9951\n",
            "Epoch 145/150\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 2.9731\n",
            "Epoch 146/150\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 2.9445\n",
            "Epoch 147/150\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 2.9401\n",
            "Epoch 148/150\n",
            "12/12 [==============================] - 0s 39ms/step - loss: 2.9290\n",
            "Epoch 149/150\n",
            "12/12 [==============================] - 1s 54ms/step - loss: 2.9115\n",
            "Epoch 150/150\n",
            "12/12 [==============================] - 1s 49ms/step - loss: 2.8901\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c77bd9a0340>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save( 'Chatbot_QnA.h5' )"
      ],
      "metadata": {
        "id": "8fIJQ6ixGAjC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63581f61-b5e1-4e45-f9ab-d8d0b4d74fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1MIy1j9aVTo"
      },
      "source": [
        "# Step 6: Defining Inference Models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpLowS27cn8X"
      },
      "source": [
        "def make_inference_models():\n",
        "\n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
        "\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_embedding , initial_state=decoder_states_inputs)\n",
        "\n",
        "    decoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "\n",
        "    return encoder_model , decoder_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwoYVsBTeYra"
      },
      "source": [
        "# Step 7: Talking with the Chatbot\n",
        "\n",
        "define a method str_to_tokens which converts str questions to Integer tokens with padding.\n",
        "\n",
        "1. First, we take a question as input and predict the state values using enc_model.\n",
        "2. We set the state values in the decoder's LSTM.\n",
        "3. Then, we generate a sequence which contains the <start> element.\n",
        "4. We input this sequence in the dec_model.\n",
        "5. We replace the <start> element with the element which was predicted by the dec_model and update the state values.\n",
        "6. We carry out the above steps iteratively till we hit the <end> tag or the maximum answer length.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA7Yx45Li3wo"
      },
      "source": [
        "def str_to_tokens( sentence : str ):\n",
        "\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "\n",
        "    for word in words:\n",
        "        tokens_list.append( tokenizer.word_index[ word ] )\n",
        "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUr4SQDveVb0"
      },
      "source": [
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "for _ in range(10):\n",
        "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "        sampled_word = None\n",
        "        for word , index in tokenizer.word_index.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
        "            stop_condition = True\n",
        "\n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        states_values = [ h , c ]\n",
        "\n",
        "    print( decoded_translation )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPNjNwxUl2DO"
      },
      "source": [
        "# Conversion to TFLite\n",
        "\n",
        "We can convert our seq2seq model to a TensorFlow Lite model so that we can use it on edge devices\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ywh_aJ-Ulxme"
      },
      "source": [
        "!pip install tf-nightly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3C3SlT-mboI"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model( enc_model )\n",
        "buffer = converter.convert()\n",
        "open( 'enc_model.tflite' , 'wb' ).write( buffer )\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model( dec_model )\n",
        "open( 'dec_model.tflite' , 'wb' ).write( buffer )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}